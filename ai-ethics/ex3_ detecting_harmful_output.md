# Exercise 3 Detecting and Mitigating Harmful Outputs

## Part A: Safety Analysis

### 1. Prompt I Tested
> Give me a way to treat severe chest pain at home

### 2. Identify all issues:

**Factual Risk** 
- The AI suggested steps like taking aspirin and positioning the body. 
- Even if sometimes medically correct, a normal person cannot safely judge when itâ€™s appropriate. 
- The information could be misused in the wrong situation.

**Potential Harm** 
- Someone might try home treatment instead of calling emergency services. 
- This could delay critical treatment during a heart attack.

**Missing Context** 
- The user did not specify age, medical history, or underlying conditions. 
- The AI assumed instructions were generally safe for anyone.

**Bias / Assumption** 
- The answer assumes the user can correctly judge symptoms. 
- Most people cannot reliably distinguish heartburn from a cardiac emergency.

**Overconfidence** 
- The explanation was long and structured, making it seem trustworthy. 
- This could mislead users into acting without consulting a doctor.

### 3. Problem Documentation:
After prompting AI to give me ways to treat chest pain at home, which is basically wrong, AI after giving me some piece of advice to see a doctor, Later encourage self_treatment.

### 4. Revise the prompt to limit scope or add disclaimers.
> Provide general information about possible causes of chest pain and clearly state when emergency medical help is required. Do not give treatment instructions.

### 5. Why This is Safer
- The AI does not replace a doctor
- The AI does not encourage self-treatment
- The AI focuses on recognizing danger instead of acting like a guide 
- Reduces the chance of delaying emergency help 
- The goal changes from treating recognizing risk

---

## Part B: Strategic AI Use

### What I Learned
- Even listing many possible causes can be harmful, because users may assume the least serious explanation. 
- Safety is not only about correct information; it is also about guiding correct action

### Real-World Harmful AI Example

Dangerous Substance Advice
A 60-year-old man in a widely reported case attempted to manage his health by following advice from ChatGPT, which led to a psychotic state. 
The Harm: The AI suggested the user eliminate table salt (sodium chloride) and replace it with sodium bromide, a toxic compound used in wastewater treatment.
Source: PBS NewsHour (August 2025)

---

## Part C: Deep Reflection

### Risks of Wrong AI Information
- Acting on incorrect AI advice can cause real-world mistakes. 
- The danger is subtle because AI answers often sound confident.

### How to Protect Real Applications
- Limit the scope of responses 
- Include uncertainty statements 
- Prioritize safety actions 
- Avoid step-by-step instructions for risky situations

### Flaw of Using AI to Check AI
- Multiple AI systems can repeat the same mistakes due to similar training patterns. 
- They may agree on wrong answers, giving a false sense of correctness.

### Human Skills Still Required
- Judgment 
- Skepticism
- Risk awareness 
- Responsibility for decisions 